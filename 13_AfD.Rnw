\documentclass[a4paper,11pt]{article}
\usepackage{graphicx} % werken met figuren
\usepackage{gensymb} % werken met wetenschappelijke eenheden\usepackage{geometry}
\usepackage{changepage} % http://ctan.org/pkg/changepage
\usepackage[total={424pt,600pt},top=100pt,left=90pt]{geometry} % instelling van de paginaindeling
\usepackage[dutch,british]{babel} % instelling van de taal (woordsplitsing, spellingscontrole)
\usepackage[parfill]{parskip} % Paragrafen gescheiden door witte lijn en geen inspringing
\usepackage{layout} % Gebruik in het begin om de layout-elementen van het document te verifiÃ«ren
\usepackage[font=small,skip=3pt]{caption} % Minder ruimte tussen figuur/table en ondertitel. Ondertitel klein
\usepackage{capt-of}
\usepackage{indentfirst}
\setlength{\parindent}{0.7cm}
\usepackage{enumitem} % Laat enumerate werken met letters
\usepackage{hyperref}
\usepackage{url}

\DeclareGraphicsExtensions{.pdf,.png,.jpg}

% Alter some LaTeX defaults for better treatment of figures:
% See p.105 of "TeX Unbound" for suggested values.
% See pp. 199-200 of Lamport's "LaTeX" book for details.
%   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.9}	% max fraction of floats at bottom
%   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \renewcommand{\textfraction}{0.1}	% allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.8}	% require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\setcounter{secnumdepth}{3}

\title{What percentage of the popular vote will the Alternative for Germany party win in Germany's next federal election?}
\author{Jan Trommelmans}

\begin{document}
\date{}
\SweaveOpts{concordance=TRUE,prefix.string=AfD}
\maketitle


<<>>=
library(rvest)
library(tidyverse)
library(lubridate)
library(stringr)
library(gridExtra)
library(TTR)
library(forecast)
@

\section{2017-08-11}

Getting the poll-results and cleaning up:

\url{https://en.wikipedia.org/wiki/Opinion_polling_for_the_German_federal_election,_2017}

<<echo=FALSE>>=
# Getting the data
writeday <- ymd("2017-08-11")
GermElurl <- "https://en.wikipedia.org/wiki/Opinion_polling_for_the_German_federal_election,_2017"
GermElurl %>% 
  read_html %>%
  html_nodes("table") -> tablelist
poll.results <- as.data.frame(html_table(tablelist[1],fill=TRUE))
# Cleaning up
# Rename vars to names of political parties
poll.results <- rename(poll.results, 
                       CDUCSU = Var.4, 
                       SPD = Var.5, 
                       DieLinke = Var.6, 
                       Grune = Var.7, 
                       FDP = Var.8, 
                       AfD =  Var.9)
# Remove (double) first row
poll.results <- poll.results[-1,]
# Get rid of the Unicode characters
n <- nrow(poll.results)
for (i in (1:n)) {
  x <- poll.results$Fieldwork.date[i]
  poll.results$Fieldwork.date[i] <- 
    ifelse(substr(x,nchar(x)-10,nchar(x)-10) %in% c("1","2","3"),
           substr(x,nchar(x)-10,nchar(x)),
           substr(x,nchar(x)-9,nchar(x)))
}
# End.date in date-format
poll.results$Fieldwork.date <- as.Date(poll.results$Fieldwork.date,format="%d %B %Y")
# Get rid of last three rows
poll.results <- poll.results[1:(n-3),]
# Keep only the data up until the day of writing this section
poll.results %>% dplyr::filter(Fieldwork.date < writeday) -> poll.results
@

Plot of AfD-polls:

<<label=AfD1,fig=TRUE,include=FALSE, echo=FALSE>>=
ggplot(data=poll.results,aes(x=Fieldwork.date)) + 
  geom_point(aes(y=AfD),colour="black") +
  stat_smooth(aes(y=AfD))
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-AfD1}
\captionof{figure}{Polling results for AfD}
\label{fig:AfD1}
\end{center}

Since May 15th the polls for AfD have been consistently below 10\% and never below 6.5\%. The range of the 95\% confidence interval is (low) 8\% and (high) 9\%.

\textbf{Forecast1: <5\%: 0\%, ]5\%-10\%]: 90\%, ]10\%-15\%]: 10\% and >=15\%: 0\%.}

\section{2017-08-16}

How good are German polls in predicting the end result? Let's try it for the Bundestag elections in 2013:

\url{https://en.wikipedia.org/wiki/Opinion_polling_for_the_German_federal_election,_2013}

<<>>=
# Getting the data
GermEl13url <- "https://en.wikipedia.org/wiki/Opinion_polling_for_the_German_federal_election,_2013"
GermEl13url %>% 
  read_html %>%
  html_nodes("table") -> tablelist
poll.results.13 <- as.data.frame(html_table(tablelist[1],fill=TRUE))
# Cleaning up
# Remove (double) first row
poll.results.13 <- poll.results.13[-1,]
# Change the name GR.U.00DC.NE to Grune
poll.results.13 <- rename(poll.results.13, Grune=GR.U.00DC.NE)
# Delete rows with year subtotals
poll.results.13 <- poll.results.13[-194,]
poll.results.13 <- poll.results.13[-404,]
poll.results.13 <- poll.results.13[-580,]
poll.results.13 <- poll.results.13[-752,]
poll.results.13 <- poll.results.13[-795,]
# Renumbering the rows
row.names(poll.results.13) <- 1:nrow(poll.results.13)
# Adding year
poll.results.13$year[1:794] <- ""
poll.results.13$year[1:193] <- "2013"
poll.results.13$year[194:403] <- "2012"
poll.results.13$year[404:579] <- "2011"
poll.results.13$year[580:751] <- "2010"
poll.results.13$year[752:794] <- "2009"
# Reformat date of first row
poll.results.13$Date[1] <- "22 Sep"
poll.results.13$Date <- paste(poll.results.13$Date,poll.results.13$year)
# Reformat Date with as.Date
poll.results.13$Date <- as.Date(poll.results.13$Date,format="%d %B %Y")
# Replace "-" with NA for PIRATEN, AfD, Others and Lead
poll.results.13$PIRATEN <- as.numeric(poll.results.13$PIRATEN)
poll.results.13$AfD <- as.numeric(poll.results.13$AfD)
poll.results.13$Others <- as.numeric(poll.results.13$Others)
poll.results.13$Lead <- as.numeric(poll.results.13$Lead)
@

Store the election results (first row) in a separate vector from the polling data, and delete the first row

<<>>=
election.result <- poll.results.13[1,]
poll.results.13 <- poll.results.13[2:nrow(poll.results.13),]
@


Tidying the data:
<<echo=FALSE>>=
poll.results.13.g <- gather(poll.results.13,"party","poll.res",3:10)
@

Getting a first picture:
<<label=2013a,fig=TRUE,include=FALSE, echo=FALSE>>=
ggplot(data=poll.results.13.g,aes(x=Date)) + 
  geom_line(aes(y=poll.res,colour=party)) 
@

\begin{center}
\includegraphics[width=0.6\textwidth]{AfD-2013a}
\captionof{figure}{Polling results for 2013 election}
\label{fig:2013a}
\end{center}

Something is wrong with the data of ''Others" at the end of September 2013. On the Wikipedia page there is an obvious error on Sept 12th: the number should be 4.0 instead of 40.

Correction:

<<>>=
poll.results.13.g$poll.res[poll.results.13.g$party=="Others" & poll.results.13.g$poll.res==40] <- 4.0 
@

<<label=2013b,fig=TRUE,include=FALSE, echo=FALSE>>=
ggplot(data=poll.results.13.g,aes(x=Date)) + 
  geom_line(aes(y=poll.res,colour=party)) 
@

\begin{center}
\includegraphics[width=0.6\textwidth]{AfD-2013b}
\captionof{figure}{Polling results for 2013 election - after correction}
\label{fig:2013b}
\end{center}

The polling period is very long (4 years) and the fortunes of the parties change considerably. Using the whole dataset to make a prediction seems wrong: the poll results closer to the election date should carry more weight. We can make predictions for the different parties based on different time scales.

\subsection{Time series}

Let's first group the results by party, year, month and summarize the mean poll result per month and turn it in a time series object \footnote{''Using R for Time Seris Analysis", \url{http://a-little-book-of-r-for-time-series.readthedocs.io/en/latest/src/timeseries.html}}. The starting point of a time series object is a \textbf{vector} of the different readings of the value in time, with a fixed time-interval and sorted from earliest to latest. The poll's are not held on fixed dates so we first calculate the mean value of the polls for a month (or a week ...). We sort them per party and make a time series starting on Oct 2009 and ending in Sep 2013, with a monthly frequency (frequency=12):

<<>>=
poll.results.13.g %>% 
  select(Date, year, party, poll.res) %>% 
  group_by(party, year, month(Date)) %>% 
  summarise(month.avg=round(mean(poll.res),2)) -> monthly.avg
monthly.avg %>% rename(maand=`month(Date)`) -> monthly.avg
monthly.avg$datum <- ymd("2009/01/01")
for (i in (1:nrow(monthly.avg))) {
  monthly.avg$datum[i] <- ymd(paste(monthly.avg$year[i],as.character(monthly.avg$maand[i]),"01",sep="/"))
}
@

<<label=TimeSeries1,fig=TRUE,include=FALSE, echo=FALSE>>=
# A time series for CDU.CSU
monthly.avg %>% filter(party=="CDU.CSU") %>% 
  select(datum, month.avg) %>% 
  arrange(datum) %>% 
  select(month.avg)-> month.CDU.CSU
month.CDU.CSU.ts <- ts(month.CDU.CSU[3], start=c(2009,10), end=c(2013,9), frequency=12)
plot.ts(month.CDU.CSU.ts)
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-TimeSeries1}
\captionof{figure}{Unsmoothed plot for CDU.CSU polls 2013}
\label{fig:AfD-TimeSeries1}
\end{center}

\subsection{Effects of smoothing}

<<label=TimeSeries2,fig=TRUE,include=FALSE, echo=FALSE>>=
# A smoothing with n=3 (three months rolling average)
month.CDU.CSU.SMA3 <- SMA(month.CDU.CSU.ts, n=3)
plot.ts(month.CDU.CSU.SMA3)
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-TimeSeries2}
\captionof{figure}{Smoothed plot(n=3) for CDU.CSU polls 2013}
\label{fig:AfD-TimeSeries2}
\end{center}

<<label=TimeSeries3,fig=TRUE,include=FALSE, echo=FALSE>>=
# A smoothing with n=8 (three months rolling average)
month.CDU.CSU.SMA8 <- SMA(month.CDU.CSU.ts, n=8)
plot.ts(month.CDU.CSU.SMA8)
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-TimeSeries3}
\captionof{figure}{Smoothed plot (n=8) for CDU.CSU polls 2013}
\label{fig:AfD-TimeSeries3}
\end{center}

\subsection{Decomposing: is there a trend? a seasonal component?}

<<label=Decompose1,fig=TRUE,include=FALSE, echo=FALSE>>=
# Decomposing
month.CDU.CSU.components <- decompose(month.CDU.CSU.ts)
plot(month.CDU.CSU.components)
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-Decompose1}
\captionof{figure}{Decomposition of CDU.CSU polls 2013}
\label{fig:AfD-Decompose1}
\end{center}

There seems to be a gradual rise (trend) and a recurring pattern (seasonality): CDU.CSU seems to do well in winter! But the effect is small: the amplitude is about 1.5\% while the trend effect is 6\%. We can remove the seasonal effect:

<<label=Decompose2,fig=TRUE,include=FALSE, echo=FALSE>>=
# Decomposing
month.CDU.CSU.components <- decompose(month.CDU.CSU.ts)
month.CDU.CSU.adj <- month.CDU.CSU.ts - month.CDU.CSU.components$seasonal
plot(month.CDU.CSU.adj)
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-Decompose2}
\captionof{figure}{Evolution of CDU.CSU polls 2013 adjusted for seasonality}
\label{fig:AfD-Decompose2}
\end{center}

\subsection{Forecasting using Exponential Smoothing}

(Read first: ''ForeCast.pdf" in folder ''ForeCast" in ''R")

\subsubsection{Simple Exponential Smoothing: if there is no trend and no seasonality: HoltWinters with beta=FALSE and gamma=FALSE}

<<label=SES1,fig=TRUE,include=FALSE, echo=FALSE>>=
CDU.CSU.ses <- HoltWinters(month.CDU.CSU.ts, beta=FALSE, gamma=FALSE)
plot(CDU.CSU.ses)
CDU.CSU.ses[["alpha"]]
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-SES1}
\captionof{figure}{Single Exponential Smoothing of CDU.CSU polls 2013}
\label{fig:AfD-SES1}
\end{center}

This simply follows the data with some delay. This is logical: when you put beta=gamma=FALSE your only remaining co{\"e}fficient is alpha, which is a single exponential smoothing of the observed values. When you do not propose a value for alpha, HoltWinters will determine that value that minimizes an error function. This value is here: $\alpha$=\Sexpr{round(CDU.CSU.ses[["alpha"]],3)}.

We can make a forecast (for 5 months: h=5) based on this model:

<<label=SES2,fig=TRUE,include=FALSE, echo=FALSE>>=
CDU.CSU.fc.ses <- forecast.HoltWinters(CDU.CSU.ses, h=5)
CDU.CSU.fc.ses
plot.forecast(CDU.CSU.fc.ses)
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-SES2}
\captionof{figure}{Single Exponential Smoothing Forecast based on CDU.CSU polls 2013}
\label{fig:AfD-SES2}
\end{center}

This method (Simple Exponential Smoothing) is not appropriate for the CDU.CSU-data which have a clear trend and seasonality. There is a check for the appropriateness of SSE. The in-sample forecast errors are stored in ''residuals" and there should be no correlation between forecast errors of successive predictions. This can be viewed with the ''acf"-function:

<<label=SESauto,fig=TRUE,include=FALSE, echo=FALSE>>=
acf(CDU.CSU.fc.ses$residuals, lag.max=20, na.action=na.omit)
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-SESauto}
\captionof{figure}{Correlation beween residuals of at different time points}
\label{fig:AfD-SESauto}
\end{center}

and tested with the ''Box"-test:

<<>>=
Box.test(CDU.CSU.fc.ses$residuals, lag=20, type="Ljung-Box")
@

The residuals should be normaly distributed:

<<label=SESres,fig=TRUE,include=FALSE, echo=FALSE>>=
res <- as.data.frame(CDU.CSU.fc.ses$residuals)$x
res[1] <- 0
qqnorm(res)
qqline(res,col="red")
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-SESres}
\captionof{figure}{Plot of residuals}
\label{fig:AfD-SESres}
\end{center}

This does not bring greater clarity!

\subsubsection{Double Exponential Smoothing: there is a trend but no seasonality: HoltWinters with gamma=FALSE}

<<label=DES1,fig=TRUE,include=FALSE, echo=FALSE>>=
CDU.CSU.des <- HoltWinters(month.CDU.CSU.ts, gamma=FALSE)
plot(CDU.CSU.des)
CDU.CSU.des[["alpha"]]
CDU.CSU.des[["beta"]]
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-DES1}
\captionof{figure}{Double Exponential Smoothing of CDU.CSU polls 2013}
\label{fig:AfD-DES1}
\end{center}

Forecast (for 5 months: h=5) based on this model:

<<label=DES2,fig=TRUE,include=FALSE, echo=FALSE>>=
CDU.CSU.fc.des <- forecast.HoltWinters(CDU.CSU.des, h=5)
CDU.CSU.fc.des
plot.forecast(CDU.CSU.fc.des)
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-DES2}
\captionof{figure}{Double Exponential Smoothing Forecast based on CDU.CSU polls 2013}
\label{fig:AfD-DES2}
\end{center}

\end{document}