\documentclass[a4paper,11pt]{article}
\usepackage{graphicx} % werken met figuren
\usepackage{gensymb} % werken met wetenschappelijke eenheden\usepackage{geometry}
\usepackage{changepage} % http://ctan.org/pkg/changepage
\usepackage[total={424pt,600pt},top=100pt,left=90pt]{geometry} % instelling van de paginaindeling
\usepackage[dutch,british]{babel} % instelling van de taal (woordsplitsing, spellingscontrole)
\usepackage[parfill]{parskip} % Paragrafen gescheiden door witte lijn en geen inspringing
\usepackage{layout} % Gebruik in het begin om de layout-elementen van het document te verifiÃ«ren
\usepackage[font=small,skip=3pt]{caption} % Minder ruimte tussen figuur/table en ondertitel. Ondertitel klein
\usepackage{capt-of}
\usepackage{indentfirst}
\setlength{\parindent}{0.7cm}
\usepackage{enumitem} % Laat enumerate werken met letters
\usepackage{hyperref}
\usepackage{url}

\DeclareGraphicsExtensions{.pdf,.png,.jpg}

% Alter some LaTeX defaults for better treatment of figures:
% See p.105 of "TeX Unbound" for suggested values.
% See pp. 199-200 of Lamport's "LaTeX" book for details.
%   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.9}	% max fraction of floats at bottom
%   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \renewcommand{\textfraction}{0.1}	% allow minimal text w. figs
%   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.8}	% require fuller float pages
% N.B.: floatpagefraction MUST be less than topfraction !!
\setcounter{secnumdepth}{3}

\title{What percentage of the popular vote will the Alternative for Germany party win in Germany's next federal election?}
\author{Jan Trommelmans}

\begin{document}
\date{}
\SweaveOpts{concordance=TRUE,prefix.string=AfD}
\maketitle


<<>>=
library(rvest)
library(tidyverse)
library(lubridate)
library(stringr)
library(gridExtra)
library(TTR)
library(forecast)
@

<<>>=
plotForecastErrors <- function(forecasterrors) {
  # make a histogram of the forecast errors:
  mybinsize <- IQR(forecasterrors)/4
  mysd <- sd(forecasterrors)
  mymin <- min(forecasterrors) - mysd*3
  mymax <- max(forecasterrors) + mysd*3
  # generate normally distributed data with mean 0 and standarddeviation mysd
  mynorm <- rnorm(10000, mean=0, sd=mysd)
  mymin2 <- min(mynorm)
  mymax2 <- max(mynorm)
  if (mymin2 < mymin) {mymin <- mymin2}
  if (mymax2 > mymax) {mymax <- mymax2}
  # make a red histogram of the forecasterrors , with the normally distributed data overlaid
  mybins <- seq(mymin, mymax, mybinsize)
  hist(forecasterrors, col="red", freq=FALSE, breaks=mybins)
  # freq=FALSE ensures the area under the histogram=1
  myhist <- hist(mynorm, plot=FALSE, breaks=mybins)
  # plot the normal curve as a blue line on top of the histogram of forecasterrors
  points(myhist$mids, myhist$density, type="l", col="blue", lwd=2)
}
@

\section{2017-08-11}

Getting the poll-results and cleaning up:

\url{https://en.wikipedia.org/wiki/Opinion_polling_for_the_German_federal_election,_2017}

<<echo=TRUE>>=
# Getting the data
writeday <- ymd("2017-08-11")
GermElurl <- "https://en.wikipedia.org/wiki/Opinion_polling_for_the_German_federal_election,_2017"
GermElurl %>% 
  read_html %>%
  html_nodes("table") -> tablelist
poll.results <- as.data.frame(html_table(tablelist[1],fill=TRUE))
# Cleaning up
# Rename vars to names of political parties
poll.results <- rename(poll.results, 
                       CDUCSU = Var.4, 
                       SPD = Var.5, 
                       DieLinke = Var.6, 
                       Grune = Var.7, 
                       FDP = Var.8, 
                       AfD =  Var.9)
# Remove (double) first row
poll.results <- poll.results[-1,]
# Get rid of the Unicode characters
n <- nrow(poll.results)
for (i in (1:n)) {
  x <- poll.results$Fieldwork.date[i]
  poll.results$Fieldwork.date[i] <- 
    ifelse(substr(x,nchar(x)-10,nchar(x)-10) %in% c("1","2","3"),
           substr(x,nchar(x)-10,nchar(x)),
           substr(x,nchar(x)-9,nchar(x)))
}
# End.date in date-format
poll.results$Fieldwork.date <- as.Date(poll.results$Fieldwork.date,format="%d %B %Y")
# Get rid of last three rows
poll.results <- poll.results[1:(n-3),]
# Keep only the data up until the day of writing this section
poll.results %>% dplyr::filter(Fieldwork.date < writeday) -> poll.results
@

Plot of AfD-polls:

<<label=AfD1,fig=TRUE,include=FALSE, echo=TRUE>>=
ggplot(data=poll.results,aes(x=Fieldwork.date)) + 
  geom_point(aes(y=AfD),colour="black") +
  stat_smooth(aes(y=AfD))
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-AfD1}
\captionof{figure}{Polling results for AfD}
\label{fig:AfD1}
\end{center}

Since May 15th the polls for AfD have been consistently below 10\% and never below 6.5\%. The range of the 95\% confidence interval is (low) 8\% and (high) 9\%.

\textbf{Forecast1: <5\%: 0\%, ]5\%-10\%]: 90\%, ]10\%-15\%]: 10\% and >=15\%: 0\%.}

\section{2017-08-16}

How good are German polls in predicting the end result? Let's try it for the Bundestag elections in 2013:

\url{https://en.wikipedia.org/wiki/Opinion_polling_for_the_German_federal_election,_2013}

<<>>=
# Getting the data
GermEl13url <- "https://en.wikipedia.org/wiki/Opinion_polling_for_the_German_federal_election,_2013"
GermEl13url %>% 
  read_html %>%
  html_nodes("table") -> tablelist
poll.results.13 <- as.data.frame(html_table(tablelist[1],fill=TRUE))
# Cleaning up
# Remove (double) first row
poll.results.13 <- poll.results.13[-1,]
# Change the name GR.U.00DC.NE to Grune
poll.results.13 <- rename(poll.results.13, Grune=GR.U.00DC.NE)
# Delete rows with year subtotals
poll.results.13 <- poll.results.13[-194,]
poll.results.13 <- poll.results.13[-404,]
poll.results.13 <- poll.results.13[-580,]
poll.results.13 <- poll.results.13[-752,]
poll.results.13 <- poll.results.13[-795,]
# Renumbering the rows
row.names(poll.results.13) <- 1:nrow(poll.results.13)
# Adding year
poll.results.13$year[1:794] <- ""
poll.results.13$year[1:193] <- "2013"
poll.results.13$year[194:403] <- "2012"
poll.results.13$year[404:579] <- "2011"
poll.results.13$year[580:751] <- "2010"
poll.results.13$year[752:794] <- "2009"
# Reformat date of first row
poll.results.13$Date[1] <- "22 Sep"
poll.results.13$Date <- paste(poll.results.13$Date,poll.results.13$year)
# Reformat Date with as.Date
poll.results.13$Date <- as.Date(poll.results.13$Date,format="%d %B %Y")
# Replace "-" with NA for PIRATEN, AfD, Others and Lead
poll.results.13$PIRATEN <- as.numeric(poll.results.13$PIRATEN)
poll.results.13$AfD <- as.numeric(poll.results.13$AfD)
poll.results.13$Others <- as.numeric(poll.results.13$Others)
poll.results.13$Lead <- as.numeric(poll.results.13$Lead)
@

Store the election results (first row) in a separate vector from the polling data, and delete the first row

<<>>=
election.result <- poll.results.13[1,]
poll.results.13 <- poll.results.13[2:nrow(poll.results.13),]
@


Tidying the data:
<<echo=TRUE>>=
poll.results.13.g <- gather(poll.results.13,"party","poll.res",3:10)
@

Getting a first picture:
<<label=2013a,fig=TRUE,include=FALSE, echo=TRUE>>=
ggplot(data=poll.results.13.g,aes(x=Date)) + 
  geom_line(aes(y=poll.res,colour=party)) 
@

\begin{center}
\includegraphics[width=0.6\textwidth]{AfD-2013a}
\captionof{figure}{Polling results for 2013 election}
\label{fig:2013a}
\end{center}

Something is wrong with the data of ''Others" at the end of September 2013. On the Wikipedia page there is an obvious error on Sept 12th: the number should be 4.0 instead of 40.

Correction:

<<>>=
poll.results.13.g$poll.res[poll.results.13.g$party=="Others" & poll.results.13.g$poll.res==40] <- 4.0 
@

<<label=2013b,fig=TRUE,include=FALSE, echo=TRUE>>=
ggplot(data=poll.results.13.g,aes(x=Date)) + 
  geom_line(aes(y=poll.res,colour=party)) 
@

\begin{center}
\includegraphics[width=0.6\textwidth]{AfD-2013b}
\captionof{figure}{Polling results for 2013 election - after correction}
\label{fig:2013b}
\end{center}

The polling period is very long (4 years) and the fortunes of the parties change considerably. Using the whole dataset to make a prediction seems wrong: the poll results closer to the election date should carry more weight. We can make predictions for the different parties based on different time scales.

\subsection{Time series}

Let's first group the results by party, year, month and summarize the mean poll result per month and turn it in a time series object \footnote{''Using R for Time Seris Analysis", \url{http://a-little-book-of-r-for-time-series.readthedocs.io/en/latest/src/timeseries.html}}. The starting point of a time series object is a \textbf{vector} of the different readings of the value in time, with a fixed time-interval and sorted from earliest to latest. The poll's are not held on fixed dates so we first calculate the mean value of the polls for a month (or a week ...). We sort them per party and make a time series starting on Oct 2009 and ending in Sep 2013, with a monthly frequency (frequency=12):

<<>>=
poll.results.13.g %>% 
  select(Date, year, party, poll.res) %>% 
  group_by(party, year, month(Date)) %>% 
  summarise(month.avg=round(mean(poll.res, na.rm=TRUE),2)) -> monthly.avg
monthly.avg %>% rename(maand=`month(Date)`) -> monthly.avg
monthly.avg$datum <- ymd("2009/01/01")
for (i in (1:nrow(monthly.avg))) {
  monthly.avg$datum[i] <- ymd(paste(monthly.avg$year[i],as.character(monthly.avg$maand[i]),"01",sep="/"))
}
@

<<label=TimeSeries1,fig=TRUE,include=FALSE, echo=TRUE>>=
# A time series for CDU.CSU
monthly.avg %>% filter(party=="CDU.CSU") %>% 
  select(datum, month.avg) %>% 
  arrange(datum) %>% 
  select(month.avg)-> month.CDU.CSU
month.CDU.CSU.ts <- ts(month.CDU.CSU[3], start=c(2009,10), end=c(2013,9), frequency=12)
plot.ts(month.CDU.CSU.ts)
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-TimeSeries1}
\captionof{figure}{Unsmoothed plot for CDU.CSU polls 2013}
\label{fig:AfD-TimeSeries1}
\end{center}

\subsection{Effects of smoothing}

<<label=TimeSeries2,fig=TRUE,include=FALSE, echo=TRUE>>=
# A smoothing with n=3 (three months rolling average)
month.CDU.CSU.SMA3 <- SMA(month.CDU.CSU.ts, n=3)
plot.ts(month.CDU.CSU.SMA3)
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-TimeSeries2}
\captionof{figure}{Smoothed plot(n=3) for CDU.CSU polls 2013}
\label{fig:AfD-TimeSeries2}
\end{center}

<<label=TimeSeries3,fig=TRUE,include=FALSE, echo=TRUE>>=
# A smoothing with n=8 (three months rolling average)
month.CDU.CSU.SMA8 <- SMA(month.CDU.CSU.ts, n=8)
plot.ts(month.CDU.CSU.SMA8)
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-TimeSeries3}
\captionof{figure}{Smoothed plot (n=8) for CDU.CSU polls 2013}
\label{fig:AfD-TimeSeries3}
\end{center}

\subsection{Decomposing: is there a trend? a seasonal component?}

<<label=Decompose1,fig=TRUE,include=FALSE, echo=TRUE>>=
# Decomposing
month.CDU.CSU.components <- decompose(month.CDU.CSU.ts)
plot(month.CDU.CSU.components)
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-Decompose1}
\captionof{figure}{Decomposition of CDU.CSU polls 2013}
\label{fig:AfD-Decompose1}
\end{center}

There seems to be a gradual rise (trend) and a recurring pattern (seasonality): CDU.CSU seems to do well in winter! But the effect is small: the amplitude is about 1.5\% while the trend effect is 6\%. We can remove the seasonal effect:

<<label=Decompose2,fig=TRUE,include=FALSE, echo=TRUE>>=
# Decomposing
month.CDU.CSU.components <- decompose(month.CDU.CSU.ts)
month.CDU.CSU.adj <- month.CDU.CSU.ts - month.CDU.CSU.components$seasonal
plot(month.CDU.CSU.adj)
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-Decompose2}
\captionof{figure}{Evolution of CDU.CSU polls 2013 adjusted for seasonality}
\label{fig:AfD-Decompose2}
\end{center}

\subsection{Forecasting using Exponential Smoothing}

(Read first: ''ForeCast.pdf" in folder ''ForeCast" in ''R")

\subsubsection{Simple Exponential Smoothing: if there is no trend and no seasonality: HoltWinters with beta=FALSE and gamma=FALSE}

<<label=SES1,fig=TRUE,include=FALSE, echo=TRUE>>=
CDU.CSU.ses <- HoltWinters(month.CDU.CSU.ts, beta=FALSE, gamma=FALSE)
plot(CDU.CSU.ses)
CDU.CSU.ses[["alpha"]]
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-SES1}
\captionof{figure}{Single Exponential Smoothing of CDU.CSU polls 2013}
\label{fig:AfD-SES1}
\end{center}

This simply follows the data with some delay. This is logical: when you put beta=gamma=FALSE your only remaining co{\"e}fficient is alpha, which is a single exponential smoothing of the observed values. When you do not propose a value for alpha, HoltWinters will determine that value that minimizes an error function. This value is here: $\alpha$=\Sexpr{round(CDU.CSU.ses[["alpha"]],3)} which is equivalent to the naive prediction method which simply says: $y_{i+1}=y_{i}$.

A test for the quality of the model is the Sum of Squared Errors (SSE):

<<>>=
CDU.CSU.ses$SSE
@

We can make a forecast (for 5 months: h=5) based on this model:

<<label=SES2,fig=TRUE,include=FALSE, echo=TRUE>>=
CDU.CSU.fc.ses <- forecast.HoltWinters(CDU.CSU.ses, h=5)
CDU.CSU.fc.ses
plot.forecast(CDU.CSU.fc.ses)
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-SES2}
\captionof{figure}{Single Exponential Smoothing Forecast based on CDU.CSU polls 2013}
\label{fig:AfD-SES2}
\end{center}

This method (Simple Exponential Smoothing) is not appropriate for the CDU.CSU-data which have a clear trend and seasonality. There is a check for the appropriateness of SSE. The in-sample forecast errors are stored in ''residuals" and there should be no correlation between forecast errors of successive predictions. This can be viewed with the ''Acf"-function from the ''forecast" package:

<<label=SESauto,fig=TRUE,include=FALSE, echo=TRUE>>=
Acf(CDU.CSU.fc.ses$residuals, lag.max=20, na.action=na.omit)
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-SESauto}
\captionof{figure}{Correlation beween residuals of at different time points}
\label{fig:AfD-SESauto}
\end{center}

There is some evidence of autocorrelation with lag=11 which is consistent with a seasonality of 12 months. A test for autocorrelation between the residuals is the ''Box"-test:

<<>>=
Box.test(CDU.CSU.fc.ses$residuals, lag=20, type="Ljung-Box")
@

The p-value is just above 5\% so we can assume some seasonality.

The residuals should also be normaly distributed:

<<label=SESres1,fig=TRUE,include=FALSE, echo=TRUE>>=
plotForecastErrors(CDU.CSU.fc.ses$residuals[-1])
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-SESres1}
\captionof{figure}{Plot of residuals}
\label{fig:AfD-SESres1}
\end{center}

or with qqnorm:

<<label=SESres2,fig=TRUE,include=FALSE, echo=TRUE>>=
res <- as.data.frame(CDU.CSU.fc.ses$residuals)$x
res[1] <- 0
qqnorm(res)
qqline(res,col="red")
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-SESres2}
\captionof{figure}{qqnorm of residuals}
\label{fig:AfD-SESres2}
\end{center}

This seem to be OK.

\subsubsection{Double Exponential Smoothing: there is a trend but no seasonality: HoltWinters with gamma=FALSE}

<<label=DES1,fig=TRUE,include=FALSE, echo=TRUE>>=
CDU.CSU.des <- HoltWinters(month.CDU.CSU.ts, gamma=FALSE)
plot(CDU.CSU.des)
CDU.CSU.des[["alpha"]]
CDU.CSU.des[["beta"]]
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-DES1}
\captionof{figure}{Double Exponential Smoothing of CDU.CSU polls 2013}
\label{fig:AfD-DES1}
\end{center}

The in-sample forecasts are rather good, although they lag the observed values.

A test for the quality of the forecast is the Sum of Squared Errors (SSE):

<<>>=
CDU.CSU.des$SSE
@

Forecast (for 5 months: h=5) based on this model:

<<label=DES2,fig=TRUE,include=FALSE, echo=TRUE>>=
CDU.CSU.fc.des <- forecast.HoltWinters(CDU.CSU.des, h=5)
CDU.CSU.fc.des
plot.forecast(CDU.CSU.fc.des)
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-DES2}
\captionof{figure}{Double Exponential Smoothing Forecast based on CDU.CSU polls 2013}
\label{fig:AfD-DES2}
\end{center}

<<label=DESauto,fig=TRUE,include=FALSE, echo=TRUE>>=
Acf(CDU.CSU.fc.des$residuals, lag.max=20, na.action=na.omit)
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-DESauto}
\captionof{figure}{Correlation beween residuals of at different time points}
\label{fig:AfD-DESauto}
\end{center}

There is some evidence of autocorrelation with lag=11 which is consistent with a seasonality of 12 months. A test for autocorrelation between the residuals is the ''Box"-test:

<<>>=
Box.test(CDU.CSU.fc.des$residuals, lag=20, type="Ljung-Box")
@

The p-value is markedly above 5\% so we can keep the null-hypotheses of no autocorrelation..

The residuals should also be normaly distributed:

<<label=DESres1,fig=TRUE,include=FALSE, echo=TRUE>>=
plotForecastErrors(CDU.CSU.fc.des$residuals[-c(1,2)])
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-DESres1}
\captionof{figure}{Plot of residuals}
\label{fig:AfD-DESres1}
\end{center}

or with qqnorm:

<<label=DESres2,fig=TRUE,include=FALSE, echo=TRUE>>=
res <- as.data.frame(CDU.CSU.fc.des$residuals)$x
res[1] <- 0
qqnorm(res)
qqline(res,col="red")
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-DESres2}
\captionof{figure}{qqnorm of residuals}
\label{fig:AfD-DESres2}
\end{center}

\subsubsection{Triple Exponential Smoothing: there is a trend and seasonality: HoltWinters with alpha, beta and gamma}

<<label=TES1,fig=TRUE,include=FALSE, echo=TRUE>>=
CDU.CSU.tes <- HoltWinters(month.CDU.CSU.ts)
plot(CDU.CSU.tes)
CDU.CSU.tes[["alpha"]]
CDU.CSU.tes[["beta"]]
CDU.CSU.tes[["gamma"]]
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-TES1}
\captionof{figure}{Triple Exponential Smoothing of CDU.CSU polls 2013}
\label{fig:AfD-TES1}
\end{center}

\begin{itemize}
  \item The value for alpha is rather high, indicating that only the present and the former observed value play a roll in predicting the next value. 
  \item The value of beta is low, which means that the slope is only adjusted very gradually. This agrees with the observation that the slope is almost constant, and thus practically equal to the initial value of the slope.
  \item Gamma is high indicating that the estimate of the seasonal component is based on the most recent observations.
\end{itemize}

The in-sample forecasts are rather good and Sum of Squared Errors (SSE) is:

<<>>=
CDU.CSU.tes$SSE
@

The best value up to now.

Forecast (for 5 months: h=5) based on this model:

<<label=TES2,fig=TRUE,include=FALSE, echo=TRUE>>=
CDU.CSU.fc.tes <- forecast.HoltWinters(CDU.CSU.tes, h=5)
CDU.CSU.fc.tes
plot.forecast(CDU.CSU.fc.tes)
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-TES2}
\captionof{figure}{Triple Exponential Smoothing Forecast based on CDU.CSU polls 2013}
\label{fig:AfD-TES2}
\end{center}

<<label=TESauto,fig=TRUE,include=FALSE, echo=TRUE>>=
Acf(CDU.CSU.fc.tes$residuals, lag.max=20, na.action=na.omit)
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-TESauto}
\captionof{figure}{Correlation beween residuals of at different time points}
\label{fig:AfD-TESauto}
\end{center}

No autocorrelation. A test for autocorrelation between the residuals is the ''Box"-test:

<<>>=
Box.test(CDU.CSU.fc.tes$residuals, lag=20, type="Ljung-Box")
@

The p-value is very high so we can keep the null-hypotheses of no autocorrelation..

The residuals should also be normaly distributed:

<<label=TESres1,fig=TRUE,include=FALSE, echo=TRUE>>=
plotForecastErrors(CDU.CSU.fc.tes$residuals[-c(1:12)])
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-TESres1}
\captionof{figure}{Plot of residuals}
\label{fig:AfD-TESres1}
\end{center}

or with qqnorm:

<<label=TESres2,fig=TRUE,include=FALSE, echo=TRUE>>=
qqnorm(CDU.CSU.fc.tes$residuals[-c(1:12)])
qqline(CDU.CSU.fc.tes$residuals[-c(1:12)],col="red")
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-TESres2}
\captionof{figure}{qqnorm of residuals}
\label{fig:AfD-TESres2}
\end{center}

\section{Let's put everything in a function}

<<>>=
HltWntrs <- function(df.ts) {
  HW <- HoltWinters(df.ts)
  paramHW <- data.frame(alpha=HW$alpha[[1]], beta=HW$beta[[1]], gamma=HW$gamma[[1]], SSE=HW$SSE)
  HW.fc <- forecast.HoltWinters(HW, h=1)
  forecastHW <- data.frame(y=HW.fc[["mean"]], lo95=HW.fc[["lower"]][1,2], hi95=HW.fc[["upper"]][1,2])
  return(list(param=paramHW,frcst=forecastHW))
}
@

<<>>=
election.result %>% select(-c(Date,Polling.Firm,Lead,year)) -> prty.res
prty.res <- prty.res[,order(names(prty.res))]
y <- election.result[,-c(3:10)]
y[,5:12] <- prty.res
mytable.param <- data.frame(party=c("O"), alpha=0, beta=0, gamma=0, SSE=0, stringsAsFactors=FALSE)
mytable.pred <- data.frame(party=c("O"), result=0, predict=0, lo95=0, hi95=0, naive=0, stringsAsFactors=FALSE)
for (i in (1:7)) {
  acro <- c("CDU.CSU", "FDP", "Grune", "LINKE", "Others", "PIRATEN", "SPD")
  monthly.avg %>% filter(party==acro[i]) %>% 
    select(party, datum, month.avg) %>% 
    arrange(datum) %>% 
    select(party, year, month.avg)-> month.prty
    if (acro[i]=="AfD") {
      month.prty <- month.prty[complete.cases(month.prty),]
      startdt <- c(2013,4)
      month.prty.ts <- ts(month.prty[,3], start=startdt, end=c(2013,9), frequency=1)
      } else {
    if (acro[i]=="PIRATEN") {
      month.prty <- month.prty[complete.cases(month.prty),]
      startdt <- c(2011,9)
      month.prty.ts <- ts(month.prty[,3], start=startdt, end=c(2013,9), frequency=12)
    } else {
      startdt <- c(2009,10)
      month.prty.ts <- ts(month.prty[,3], start=startdt, end=c(2013,9), frequency=12)
    }
      }
  # Storing the timeseries per party in time series object with the party name
  var.name <- acro[i]
  assign(var.name,month.prty.ts)
  #
  HltWntrs(month.prty.ts)
  mytable.param[i,1] <- acro[i]
  mytable.param[i,2:5] <- HltWntrs(month.prty.ts)$param
  mytable.pred[i,1] <- acro[i]
  mytable.pred[i,2] <- y[i+5]
  mytable.pred[i,3:5] <- round(HltWntrs(month.prty.ts)$frcst,2)
  mytable.pred[i,6] <- month.prty[nrow(month.prty),3]
}
  print(mytable.param)
  mytable.pred$diff.HW <- mytable.pred$predict - mytable.pred$result
  mytable.pred$diff.naive <- mytable.pred$naive - mytable.pred$result
  print(mytable.pred)
  SSE.HW <- sum(mytable.pred$diff.HW^2)
  SSE.naive <- sum(mytable.pred$diff.naive^2)
  SSE.HW
  SSE.naive
@

\textbf{Conclusion}: the polls predict the results well: they all fall within the 95\% confidence interval. Fancy prediction methods like HoltWinters with the full complement of alpha, beta and gamme are not necessary. The naive method, based on the average of the last month of polling before the election gives a result that is as good. A last try could be a HoltWinters with alpha and beta, and gamma=FALSE based on the, non averaged, results of the last days of polling. However this is problematic because getting these data into a time series is difficult because the frequency is not known. Maybe a simple linear model?

\section{Linear model based on data of last month of polling}

<<>>=
linmod.pred <- data.frame(party=c("O"), result=0, predict=0, diff.lm=0, stringsAsFactors=FALSE)
for (i in (1:8)) {
  acro <- c("CDU.CSU", "SPD", "FDP", "LINKE", "Grune", "PIRATEN", "AfD", "Others")
  poll.results.13 %>% select(Date, i+2) %>% filter(year(Date)==2013 & month(Date)==9) -> x
  colnames(x)[2] <- "poll"
  fit.x <- lm(poll ~ Date, data=x, na.action=na.omit)
  new.x <- data.frame(Date=as.Date(c("2013-09-21","2013-09-22")))
  linmod.pred[i,1] <- acro[i]
  linmod.pred[i,2] <- election.result[i+2]
  linmod.pred[i,3] <- predict(fit.x, new.x)[2]
  linmod.pred[i,4] <- linmod.pred[i,2] - linmod.pred[i,3]
}
linmod.pred
SSE.lm <- sum(linmod.pred$diff.lm^2)
SSE.lm
@

No: the naive model based on monthly average is the best!

\section{2013-09-01}

<<echo=TRUE>>=
# Getting the data
writeday <- ymd("2017-09-01")
GermElurl <- "https://en.wikipedia.org/wiki/Opinion_polling_for_the_German_federal_election,_2017"
GermElurl %>% 
  read_html %>%
  html_nodes("table") -> tablelist
poll.results <- as.data.frame(html_table(tablelist[1],fill=TRUE))
# Cleaning up
# Rename vars to names of political parties
poll.results <- rename(poll.results, 
                       CDUCSU = Var.4, 
                       SPD = Var.5, 
                       DieLinke = Var.6, 
                       Grune = Var.7, 
                       FDP = Var.8, 
                       AfD =  Var.9)
# Remove (double) first row
poll.results <- poll.results[-1,]
# Get rid of the Unicode characters
n <- nrow(poll.results)
for (i in (1:n)) {
  x <- poll.results$Fieldwork.date[i]
  poll.results$Fieldwork.date[i] <- 
    ifelse(substr(x,nchar(x)-10,nchar(x)-10) %in% c("1","2","3"),
           substr(x,nchar(x)-10,nchar(x)),
           substr(x,nchar(x)-9,nchar(x)))
}
# End.date in date-format
poll.results$Fieldwork.date <- as.Date(poll.results$Fieldwork.date,format="%d %B %Y")
# Get rid of last three rows
poll.results <- poll.results[1:(n-3),]
# Keep only the data up until the day of writing this section
poll.results %>% dplyr::filter(Fieldwork.date < writeday) -> poll.results
@

Plot of AfD-polls:

<<label=AfD2,fig=TRUE,include=FALSE, echo=TRUE>>=
ggplot(data=poll.results,aes(x=Fieldwork.date)) + 
  geom_point(aes(y=AfD),colour="black") +
  stat_smooth(aes(y=AfD))
@

\begin{center}
\includegraphics[width=0.5\textwidth]{AfD-AfD2}
\captionof{figure}{Polling results for AfD - 01.09.2017}
\label{fig:AfD2}
\end{center}

The average for the last month (august) is:

<<>>=
poll.results %>% select(Fieldwork.date, AfD) %>% filter(month(Fieldwork.date)==8) -> last.month.AfD
n <- nrow(last.month.AfD)
xbar <- mean(last.month.AfD$AfD)
saccent <- sd(last.month.AfD$AfD)
alfa <- 0.05
t0 <- qt(1-alfa/2,n-1)
low95 <- xbar - t0*saccent/sqrt(n)
high95 <- xbar + t0*saccent/sqrt(n)
low95
xbar
high95
@

\textbf{Forecast overanderd: <5\%: 0\%, ]5\%-10\%]: 90\%, ]10\%-15\%]: 10\% and >=15\%: 0\%.}

\end{document}